{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Devops Knowledge Sharing","text":""},{"location":"blog/","title":"Devops Knowledge Sharing","text":""},{"location":"blog/2024/11/14/kafka-offsets/","title":"Kafka Offsets","text":"<p>Kafka offsets are critical to understanding and managing messages consumption in Apache Kafka. An Offset is a unique identifier for each message in a kafka partition, and it tells a consumer where to continue reading if it stops and restarts.</p> <p>Let's dive into Kafka offsets with real-time examples to clarify their purpose and importance.</p>"},{"location":"blog/2024/11/14/kafka-offsets/#1-kafka-offsets-explained","title":"1. Kafka Offsets Explained","text":"<p>In Kafka, each topic is split into Partitions. Each partition is an ordered sequence of messages, and each message in a partition has a unique identifier called an offset.</p> <ul> <li> <p>Offset: An integer value assigned to messages in a partition, identifying their position. The first message in a partition might have an offset of 0, the next one 1, and so on.</p> </li> <li> <p>Consumer Offset: This is the offset that a consumer reads from in a partition. Kafka allows consumers to commit offsets, so they can resume reading from where they left off, even after a restart.</p> </li> </ul>"},{"location":"blog/2024/11/14/kafka-offsets/#2-why-kafka-offsets-are-important","title":"2. Why Kafka Offsets Are Important","text":"<p>Offsets are essential for tracking and replaying messages. Here's why:</p> <ul> <li> <p>Restarting Consumers: If a consumer crashes, it can resume from the last committed offset, avoiding message duplication or message loss.</p> </li> <li> <p>Replaying Messages: You can rewind to a specific offset to replay messages for specific offset to replay messages for analytics or troubleshooting purposes.</p> </li> <li> <p>Tracking Progress: Offsets help track which messages have been processed, allowing for fault tolarence.</p> </li> </ul>"},{"location":"blog/2024/11/14/kafka-offsets/#3-real-time-examples-of-kafka-offset-management","title":"3. Real-Time Examples of Kafka Offset Management","text":"<p>Example 1: Basic Offset Tracking in a Consumer Group</p> <p>Imagine you have a kafka topic with a single partition, and a consumer group with two consumers, Consumer-A and Consumer-B, is reading messages. Here's how offsets work in this scenario:</p> <ol> <li> <p>Message Arrival: Messages m0, m1, and m2 are produced to the partition with offsets 0, 1, and 2, respectively.</p> </li> <li> <p>Consumer Read: Consumer-A reads m0 and commits the offset 0. Then it reads m1 and commits offset 1.</p> </li> <li> <p>Failure and Recovery: If Consumer-A crashes after reading m1, it can resume at offset 2 (last commit offset) upon restart. This avoids reprocessing m1 or missing m2.</p> </li> </ol> <p>This setup helps ensure no message duplication or loss. </p> <p>Example 2: Tracking and Committing Offsets Manually</p> <p>Considering a situation where you're processing critical messages, and you want precise control over when offsets are committed. In thic case, you might manually commit offsets after each message processing, rather than automatically.</p> <ol> <li> <p>Start Reading: Consumer-B reads a message with offset 0 but doesn't immediately commit it.</p> </li> <li> <p>Processing: It takes some time to process this message. If the consumer crashes before commiting the offset, it will resume at 0, re-reading the message upon restart.</p> </li> <li> <p>Commit After Processing: After processing, Consumer-B commits offset 0, ensuring progress is saved only after successful processing.</p> </li> </ol> <p>Manualy commits give more control and can be essential in use cases where processing integrity is critical.</p>"},{"location":"blog/2024/11/14/kafka-offsets/#4-real-time-example-replay-and-rewind-using-offsets","title":"4. Real-Time Example: Replay and Rewind Using Offsets","text":"<p>kafka's offset system allows you to replay of rewind messages. Suppose you have a kafka topic with customer purchase events, and you want to analyze event from a sepcific time.</p> <ol> <li> <p>Set Offset Manually: You can configure a consumer to start at specific offset (e.g offset 100) instead of latest message. This allows the consumer to replay messages starting from that point.</p> </li> <li> <p>Rewind: If an error occurs in the data processing logic, you can rewind to an earlier offset and reprocess the messages, correcting any issues with out reading to republish events.</p> </li> </ol> <p>This replay and rewind capability make Kafka a great choice for event-driven applications, where historical data needs to be processed.</p>"},{"location":"blog/2024/11/14/kafka-offsets/#5-implementing-offset-tracking-with-a-real-time-python-example","title":"5. Implementing Offset Tracking with a Real-Time Python Example","text":"<p>Here's python script that demonstrates real-time Kafka offset tracking using the kafka-python library. This script consumes messages from a kafka topic and logs the offset of each message, allowing you to track progress and replay messages if needed.</p> <p>Python Code for Real-Time Offset Tracking</p> <pre><code>from kafka import kafkaConsumer\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n\ndef consume_messages(broker, topic):\n  consumer = kafkaConsumer(\n    topic,\n    bootstrap_servers=[broker],\n    group_id=\"example_group\",\n    auto_offset_reset=\"earliest\", # Start from the beginning if no commited offset\n    enable_auto_commit=False      # Manual offset handling\n  )\n\n  try:\n    for message in consumer:\n      # Display the message and its offset\n      print(f\"Message: {message.value.decode('utf-8')}\")\n      print(f\"Partition: {message.Partition}, Offset: {message.offset}\")\n\n      # Log offset info for demonstration purpose only\n      logging.info(f\"Processes message at offset {message.offset} in partition {message.partition}\")\n\n      # Commit offset after processing each message\n      consumer.commit()\n  except KeyboardInterrupt:\n    print(\"Stopped Consuming!\")\n  finally:\n    consumer.close()\n\n# Configure and run the consumer\nbroker = \"localhost:9092\"\ntopic  = \"your_topic_name\"\nconsume_messages(broker, topic)\n</code></pre> <p>Explanation of the Code</p> <ul> <li> <p>Kafka Consumer Setup: Connects to a Kafka broker and reads from a specified topic.</p> </li> <li> <p>Manual Offset Commit: Offsets are committed manually using <code>consumer.commit()</code> to ensure each offset is recorded only after successful messsage processing.</p> </li> <li> <p>Logging: Each message and its offset are printed and logged, allowing you to track real-time processing.</p> </li> </ul> <p>You can run this script and see the offset in real time. Restarting the consumer should resume reading from the last committed offset, demonstrating the importance of offsets for fault-tolerant processing.</p>"},{"location":"blog/2024/11/14/kafka-offsets/#conclusion","title":"Conclusion","text":"<p>Kafka offsets are foundational to reliable message consumption and processing. They enable consumers to:</p> <ul> <li> <p>Track progress</p> </li> <li> <p>Rewind and replay messages</p> </li> <li> <p>Manage message processing with fault tolerance</p> </li> </ul> <p>Understanding offsets and managing them properly can enhance Kafka applications' robustness, making them more resilient and reliable in production.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/","title":"Emqx and Kafka app","text":"<p>The goal of this document is to use Kafka to process massive amounts of data from Internet of Things (IOT) devices in an efficient manner. However, since Kafka was not intended for IOT devices, the integration of EMQX\u2014which uses the MQTT lightweight protocol\u2014with Kafka can elevate a variety of opportunities to process the data in real-time, greatly benefiting time-sensitive applications.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#introduction","title":"Introduction","text":""},{"location":"blog/2023/11/13/emqx-and-kafka-app/#emqx","title":"EMQX","text":"<p>EMQX is an open-source, highly scalable, and feature-rich MQTT broker designed for IoT and real-time messaging applications. It supports up to 100 million concurrent IoT device connections per cluster while maintaining a throughput of 1 million messages per second and a millisecond latency.</p> <p>EMQX supports various protocols, including MQTT (3.1, 3.1.1, and 5.0), HTTP, QUIC, and WebSocket. It also provides secure bi-directional communication with MQTT over TLS/SSL and various authentication mechanisms, ensuring reliable and efficient communication infrastructure for IoT devices and applications.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#kafka","title":"KAFKA","text":"<p>Apache Kafka is a widely used open-source distributed event streaming platform that can handle the real-time transfer of data streams between applications and systems. However, Kafka is not built for edge IoT communication and Kafka clients require a stable network connection and more hardware resources. In the IoT realm, data generated from devices and applications are transmitted using the lightweight MQTT protocol. EMQX\u2019s integration with Kafka/Confluent enables users to stream MQTT data seamlessly into or from Kafka. MQTT data streams are ingested into Kafka topics, ensuring real-time processing, storage, and analytics. Conversely, Kafka topics data can be consumed by MQTT devices, enabling timely actions.</p> <p>We will accomplish the following architecture in this tutorial.</p> <p></p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#setting-up-emqx","title":"Setting up EMQX","text":""},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-1-running-emqx-service","title":"Step 1: Running EMQX service","text":"<p>When setting up the emqx server locally, Docker is a really helpful tool. It can remove a lot of steps and allow us to run the emqx server configuration with just two commands, as shown below.</p> <pre><code>docker pull emqx/emqx\ndocker run -d --name emqx -p 1883:1883 -p 8083:8083 -p 8084:8084 -p 8883:8883 -p 18083:18083 emqx/emqx\n</code></pre> <p>We can also pull and execute the emqx docker image on the Windows computer by using docker desktop.</p> <p></p> <p>EMQX supports the following message transmission protocols, and once you run the docker image, you should be able to see the logs that contain the details about the ports on which emqx services are running. The listener in EMQX is set up to receive requests from MQTT clients.</p> <pre><code>- TCP: port 1883\n- SSL: port 8883\n- Websocket listener: 8083\n- Secure websocket listener: 8084\n- For UI Dashboard: 18083\n</code></pre> <p></p> <p>In EMQX, Dashboard is a web-based graphic interface to manage and monitor EMQX and connected devices in real time.  Access the endpoint <code>&lt;ipaddress&gt;:18083</code></p> <p></p> <p>Login into the dashboard using default credentials.</p> <pre><code>username: admin\npassword: public\n</code></pre> <p>Upon logging in for the first time, you will be prompted to reset your password, which will then take you to the emqx dashboard.</p> <p></p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-2-create-emqx-publishersubscriber-files-using-python-programming","title":"Step 2: Create emqx publisher/subscriber files using python programming *** End of EMQX setup ***","text":""},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-1-using-paho-mqtt-python-module","title":"Step 1: using paho-mqtt python module","text":"<p>This article primarily explains how to use the paho-mqtt client and construct message, connection, and subscription functions in Python between the MQTT client and MQTT broker.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-2-install-the-paho-mqtt-client","title":"Step 2: Install the Paho Mqtt Client","text":"<p><code>pip3 install paho-mqtt</code></p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-3-create-a-emqx-python-publisher","title":"Step 3: Create a emqx python publisher.","text":"<p>``` from paho.mqtt import client as mqtt_client import random import logging import time</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#create-an-mqtt-connection","title":"Create an MQTT Connection","text":"<p>broker      = '192.168.48.1' port        = 1883 topic       = \"python/mqtt\" client_id   = f'python-mqtt-{random.randint(0, 1000)}' username    = \"admin\" password    = \"public\"</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#auto-reconnect-for-reliable-connection","title":"Auto reconnect for reliable connection","text":"<p>FIRST_RECONNECT_DELAY   = 1 RECONNECT_RATE          = 2 MAX_RECONNECT_COUNT     = 12 MAX_RECONNECT_DELAY     = 60</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#on_connect-callback-function-for-connecting-the-broker","title":"on_connect callback function for connecting the broker.","text":""},{"location":"blog/2023/11/13/emqx-and-kafka-app/#this-function-is-called-after-client-call-has-successfully-connected","title":"This function is called after client call has successfully connected.","text":"<p>def connect_mqtt():     def on_connect(client, userdata, flags, rc):         if rc == 0:             print(\"Connected to MQTT Broker!\")         else:             print(\"Failed to connect, return code %d\\n\", rc)</p> <pre><code>def on_disconnect(client, userdata, rc):\n    logging.info(\"Disconnected with result code: %s\", rc)\n    reconnect_count, reconnect_delay = 0, FIRST_RECONNECT_DELAY\n    while reconnect_count &lt; MAX_RECONNECT_COUNT:\n        logging.info(\"Reconnecting in %d seconds...\", reconnect_delay)\n        time.sleep(reconnect_delay)\n\n        try:\n            client.reconnect()\n            logging.info(\"Reconnected successfully!\")\n            return\n        except Exception as err:\n            logging.error(\"%s. Reconnect failed. Retrying...\", err)\n\n        reconnect_delay *= RECONNECT_RATE\n        reconnect_delay = min(reconnect_delay, MAX_RECONNECT_DELAY)\n        reconnect_count += 1\n    logging.info(\"Reconnect failed after %s attempts. Exiting...\", reconnect_count)\n\n# Set connecting Client ID\nclient = mqtt_client.Client(client_id)\nclient.username_pw_set(username, password)\nclient.on_connect = on_connect\nclient.connect(broker, port)\nclient.on_disconnect = on_disconnect\nreturn client\n</code></pre>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#publisher-code","title":"publisher code","text":"<p>def publish(client):     msg_count = 1     while True:         time.sleep(1)         msg = f\"messages: {msg_count}\"         result = client.publish(topic, msg)         status = result[0]         if status == 0:             print(f\"Send <code>{msg}</code> to topic <code>{topic}</code>\")         else:             print(f\"Failed to send message to topic {topic}\")         msg_count += 1         if msg_count &gt; 5:             break</p> <p>def run():     client = connect_mqtt()     client.loop_start()     publish(client)     client.loop_forever()</p> <p>if name == \"main\":     run() ```</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-4-create-a-emqx-python-subscriber","title":"Step 4: Create a emqx python subscriber.","text":"<p>``` import random from paho.mqtt import client as mqtt_client</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#create-an-mqtt-connection_1","title":"Create an MQTT Connection","text":"<p>broker      = '192.168.48.1' port        = 1883 topic       = \"python/mqtt\" client_id   = f'subscribe-{random.randint(0, 1000)}' username    = \"admin\" password    = \"public\"</p> <p>def connect_mqtt() -&gt; mqtt_client:     def on_connect(client, userdata, flags, rc):         if rc == 0:             print(\"Connected to MQTT Broker!\")         else:             print(\"Failed to connect, return code %d\\n\", rc)</p> <pre><code>client = mqtt_client.Client(client_id)\nclient.username_pw_set(username, password)\nclient.on_connect = on_connect\nclient.connect(broker, port)\nreturn client\n</code></pre> <p>def subscribe(client: mqtt_client):     def on_message(client, userdata, msg):         print(f\"Received <code>{msg.payload.decode()}</code> from <code>{msg.topic}</code> topic\")</p> <pre><code>client.subscribe(topic)\nclient.on_message = on_message\n</code></pre> <p>def run():     client = connect_mqtt()     subscribe(client)     client.loop_forever()</p> <p>if name == \"main\":     run() ```</p> <p>NOTE: These two files are quite basic; the publisher will send five messages before stopping, and the subscriber will be in an endless loop of receiving messages from the publisher. You can modify the publisher logic to transmit messages continuously by deleting the block below. <code>msg_count += 1     if msg_count &gt; 5:         break</code></p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-5-execute-the-subscriber-and-publisher","title":"Step 5: Execute the subscriber and Publisher","text":"<p>Run the publisher program in a separate terminal session after completing the subscriber program in the first one. The result should look somewhat like this:</p> <p></p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#setting-up-kafka-assuming-on-ubuntu-platform","title":"Setting up KAFKA (Assuming on Ubuntu platform)","text":"<p>For those who are unfamiliar with Kafka, the diagram below provides a high-level explanation of the concept. (there is lot more to Kafka, such Zookeeper, Consumer Groups, Partitions, etc., but we will leave it for another time.)</p> <p></p> <p>Kafka divides data into topics, which are categories or feed names to which records are published. Producers publish messages to a particular topic. These messages can take any format; popular formats are JSON and Avro. For instance, on a social media platform, a producer may publish messages to a topic called posts each time a user creates a post. Consumers subscribe to a topic in order to consume the records published by producers. In the example given for the social media platform, there may be a consumer set up to consume the posts topic in order to verify the post's safety before it is published to the global feed, and another consumer may asynchronously send notifications to the user's followers.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-1-installing-java","title":"Step 1: Installing Java","text":"<p>Since Oracle Java is now commercially accessible, we are using its open-source version OpenJDK. Apache Kafka can be operated on any platform that supports Java. To install Kafka on an Ubuntu server, follow these steps.</p> <pre><code>sudo apt update \nsudo apt install default-jdk\n</code></pre> <pre><code>java --version\nopenjdk 17.0.9 2023-10-17\nOpenJDK Runtime Environment (build 17.0.9+9-Ubuntu-122.04)\nOpenJDK 64-Bit Server VM (build 17.0.9+9-Ubuntu-122.04, mixed mode, sharing)\n</code></pre>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-2-download-latest-apache-kafka","title":"Step 2: Download latest apache kafka","text":"<pre><code>wget https://dlcdn.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz\n</code></pre> <p>Extract the archive file,</p> <pre><code>tar -xzf kafka_2.13-3.6.1.tgz\nsudo mv kafka_2.13-3.6.1 /usr/local/kafka\n</code></pre>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-3-running-kafka-service","title":"Step 3: Running Kafka service.","text":"<p>Now that we have two options for starting the Kafka service, we can choose to run it as a unix service. Alternatively, we can navigate to /usr/local/kafka/bin and run the Kafka script each time.</p> <p>NOTE: ZooKeeper or KRaft can be used to start Apache Kafka; in this post, we will use ZooKeeper Flow rather of both.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-1-creating-system-unit-files","title":"Step 1 - Creating System Unit Files","text":"<p><code>sudo nano /etc/systemd/system/zookeeper.service</code></p> <p>And add the following content:</p> <p>``` [Unit] Description=Apache Zookeeper server Documentation=http://zookeeper.apache.org Requires=network.target remote-fs.target After=network.target remote-fs.target</p> <p>[Service] Type=simple ExecStart=/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties ExecStop=/usr/local/kafka/bin/zookeeper-server-stop.sh Restart=on-abnormal</p> <p>[Install] WantedBy=multi-user.target ```</p> <p>Save the file and close it.</p> <p>Next, to create a system unit file for the kafka service:</p> <p><code>sudo nano /etc/systemd/system/kafka.service</code></p> <p>Add the following content:</p> <p>``` [Unit] Description=Apache Kafka Server Documentation=http://kafka.apache.org/documentation.html Requires=zookeeper.service</p> <p>[Service] Type=simple Environment=\"JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-amd64\" ExecStart=/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh</p> <p>[Install] WantedBy=multi-user.target ```</p> <p>Reload the systemd daemon to apply new changes.</p> <p><code>sudo systemctl daemon-reload</code></p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-2-start-kafka-and-zookeeper-service","title":"Step 2 - Start Kafka and Zookeeper Service","text":"<p>To start Kafka, you must first start the ZooKeeper service. To start a single-node ZooKeeper instance, use the systemctl command.</p> <p><code>sudo systemctl start zookeeper</code></p> <p>Launch the Kafka server now to see the current status:</p> <p><code>sudo systemctl start kafka sudo systemctl status kafka</code></p> <p>``` devops@LAPTOP-JF1PIDLR:~/Documents/Personal/projects/General/emqx-kafka-mongo-messaging-app$ sudo vi /etc/systemd/system/kafka.service \u25cf kafka.service - Apache Kafka Server  Loaded: loaded (/etc/systemd/system/kafka.service; disabled; vendor preset: enabled)  Active: active (running) since Tue 2023-12-12 13:41:12 EST; 6s ago    Docs: http://kafka.apache.org/documentation.html    Main PID: 50201 (java)   Tasks: 83 (limit: 9396)  Memory: 315.5M  CGroup: /system.slice/kafka.service          \u2514\u250050201 /usr/lib/jvm/java-1.17.0-openjdk-amd64/bin/java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -D&gt;</p> <p>Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,600] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient) Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,614] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread) Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,634] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer) Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,638] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor) Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,651] INFO Kafka version: 3.6.1 (org.apache.kafka.common.utils.AppInfoParser) Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,652] INFO Kafka commitId: 5e3c2b738d253ff5 (org.apache.kafka.common.utils.AppInfoParser) Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,652] INFO Kafka startTimeMs: 1702406477646 (org.apache.kafka.common.utils.AppInfoParser) Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,653] INFO [KafkaServer id=0] started (kafka.server.KafkaServer) Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,843] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node LAPTOP-JF1PIDLR.:9092 (&gt; Dec 12 13:41:17 LAPTOP-JF1PIDLR kafka-server-start.sh[50201]: [2023-12-12 13:41:17,861] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node LAPTOP-JF1PIDLR.:9 ```</p> <p>This completes the installation of Kafka. The next section of the tutorial will assist you in interacting with the Kafka server.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-4-create-a-topic-in-kafka","title":"Step 4: Create a Topic in Kafka","text":"<p>To begin working with Kafka, create a topic called \"emqx-to-kafka\" with a single partition and a single replica. Kafka provides several pre-built shell scripts for this purpose.</p> <pre><code>cd /usr/local/kafka\nbin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic emqx-to-kafka\n\nCreated topic emqx-to-kafka.\n</code></pre> <p>You can run the same command above to create multiple topics. After that, you can run the following command to see the created topics on Kafka. The replication factor specifies how many copies of data will be created. Since we are running with a single instance, keep this value 1. Set the partition options as the number of brokers you want your data to be split between.</p> <pre><code>devops@LAPTOP-JF1PIDLR:/usr/local/kafka$ bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n\nemqx-to-kafka\n</code></pre> <p>Moreover, you can use the describe command to view information like the new topic's partition count:</p> <pre><code>devops@LAPTOP-JF1PIDLR:/usr/local/kafka$ bin/kafka-topics.sh --describe --topic emqx-to-kafka --bootstrap-server localhost:9092\nTopic: emqx-to-kafka    TopicId: BPDpQiOvQ2WDGWPn45d99w PartitionCount: 1       ReplicationFactor: 1    Configs: \n        Topic: emqx-to-kafka    Partition: 0    Leader: 0       Replicas: 0     Isr: 0\n</code></pre>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-4-writing-event-into-kafka-topic","title":"Step 4: Writing event into Kafka topic","text":"<p>Run the producer client to write a few events into your topic. By default, each line you enter will result in a separate event being written to the topic. A Kafka client communicates with the Kafka brokers via the network for writing (or reading) events. Once the brokers receive the events, they will store them for as long as you need\u2014even forever.</p> <pre><code>bin/kafka-console-producer.sh --topic emqx-to-kafka --bootstrap-server localhost:9092\n&gt;\n</code></pre> <p>NOTE: In this case, <code>&gt;</code> indicates that it entered a prompt and requested that you add messages, which will be transmitted to the designated topic and the Kafka broker.</p> <p>Example instance,</p> <pre><code>devops@LAPTOP-JF1PIDLR:/usr/local/kafka$ bin/kafka-console-producer.sh --topic emqx-to-kafka --bootstrap-server localhost:9092\n&gt;\n&gt;First kafka producer message\n&gt;Second Kafka producer message\n&gt;\n</code></pre> <p>Ctrl-C can be used to end the producer client at any moment.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-5-reading-the-eventn-from-kafka-topic","title":"Step 5: Reading the eventn from Kafka topic","text":"<p>To read the events you just created, open a second terminal session and launch the consumer client:</p> <pre><code>devops@LAPTOP-JF1PIDLR:/usr/local/kafka$ bin/kafka-console-consumer.sh --topic emqx-to-kafka --from-beginning --bootstrap-server localhost:9092\n\nFirst kafka producer message\nSecond Kafka producer message\n</code></pre> <p>The consumer client can be stopped at any moment by using Ctrl-C. It began reading the buffered messages as soon as you hit the consumer command.</p> <p>Feel free to get hands-on: for instance, write more events by switching back to your producer terminal (previous step), and observe how the events appear instantly in your consumer terminal. Events in Kafka are durably stored, so you can read them as many times as you would like by opening another terminal session and repeating the previous command.</p> <p>NOTE: An option in the kakfa to select the location to read the messages from makes it very important to ensure consistency with data and avoid redundant records in time-sensitive applications.    <code>--offset earlier/latest</code></p> <p>The default value for this flag is latest, which implies that Kafka will read only messages with the most recent offset value if you do not specify it. The alternative is earliest, which indicates that Kafka will read messages from the offset where it read the most recent message.</p> <p>For instance, if we assume 0 1 2 and 3 4 5 are the producer messages and assume kafka red and processed 0 1 2. Later, due to some glitch, kafka service was restarted. In case of \"latest,\" it will wait for new messages to come in and skip 3 4 5 messages; in case of \"earliest,\" it will start reading from 3 4 5 and then latest</p> <p>In the event that there is a lag in the kafka, the application may receive outdated entries if it is expecting real-time data and is using the earlier option.</p> <p>Thus far, we have examined how to set up Kafka and handle messages using the CLI. The next step involves sending and receiving Kafka messages using the Python programming language.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-6-kafka-producer-python-module","title":"Step 6: Kafka producer python module","text":"<pre><code>import json\nfrom logging import log\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\n\n# produce json messages\n# configure multiple retries\n# produce asynchronously with callbacks\nproducer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n                         retries=5,\n                         value_serializer=lambda m: json.dumps(m).encode('ascii'))\n\ndef on_send_success(record_metadata):\n    print(\"%s:%d:%d\" % (record_metadata.topic, \n                                 record_metadata.partition,\n                                 record_metadata.offset))\n\ndef on_send_error(excp):\n    log.error('I am an errback', exc_info=excp)\n    # handle exception\n\nfor item in range(10):\n    producer.send('emqx-to-kafka', {item: 'awesome-' + str(item**2)}).add_callback(on_send_success).add_errback(on_send_error)\n\n# block until all async messages are sent\nproducer.flush()\n</code></pre>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#step-7-kafka-consumer-python-module","title":"Step 7: Kafka consumer python module *** End of Kafka setup ***","text":"<pre><code>import json\nfrom kafka import KafkaConsumer\n\n# To consume latest messages and auto-commit offsets\n# consume json messages\n# StopIteration if no message after 1sec\n# auto_offset_reset='earliest', enable_auto_commit=False\nconsumer = KafkaConsumer(bootstrap_servers='localhost:9092',\n                         auto_offset_reset='latest', \n                         enable_auto_commit=True,\n                         consumer_timeout_ms=1000,\n                         value_deserializer=lambda m: json.loads(m.decode('ascii')))\n\n# Subscribe to a regex topic pattern\nconsumer.subscribe(pattern='^emqx.*')\n\nwhile True:\n    for message in consumer:\n        # message value and key are raw bytes -- decode if necessary!\n        # e.g., for unicode: `message.value.decode('utf-8')`\n        print (\"%s:%d:%d: data=%s\" % (message.topic, message.partition,\n                                            message.offset, message.value))\n</code></pre>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#emqx-kafka-together","title":"EMQX + KAFKA together","text":"<p>After learning how EMQX and Kafka function separately up till now, it is time to combine the two to achieve better outcomes. To do this, simply combine the logics of EMQX subscribers and Kafka producers. The combined output will look like this:</p> <pre><code>import random\nimport json\nfrom logging import log\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nfrom paho.mqtt import client as mqtt_client\n\n# Create an MQTT Connection\nbroker      = '192.168.48.1'\nport        = 1883\ntopic       = \"python/mqtt\"\nclient_id   = f'subscribe-{random.randint(0, 1000)}'\nusername    = \"admin\"\npassword    = \"public\"\n\n# produce json messages\n# configure multiple retries\n# produce asynchronously with callbacks\nproducer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n                         retries=5,\n                         value_serializer=lambda m: json.dumps(m).encode('ascii'))\n\ndef connect_mqtt() -&gt; mqtt_client:\n    def on_connect(client, userdata, flags, rc):\n        if rc == 0:\n            print(\"Connected to MQTT Broker!\")\n        else:\n            print(\"Failed to connect, return code %d\\n\", rc)\n\n    client = mqtt_client.Client(client_id)\n    client.username_pw_set(username, password)\n    client.on_connect = on_connect\n    client.connect(broker, port)\n    return client\n\ndef subscribe(client: mqtt_client):\n    def on_message(client, userdata, msg):\n        print(f\"Received `{msg.payload.decode()}` from `{msg.topic}` topic\")\n        producer.send('emqx-to-kafka', {\"data\": msg.payload.decode()}).add_callback(on_send_success).add_errback(on_send_error)\n        # block until all async messages are sent\n        producer.flush()\n\n    client.subscribe(topic)\n    client.on_message = on_message\n\ndef run():\n    client = connect_mqtt()\n    subscribe(client)\n    client.loop_forever()\n\ndef on_send_success(record_metadata):\n    print(\"%s:%d:%d\" % (record_metadata.topic, \n                                 record_metadata.partition,\n                                 record_metadata.offset))\n\ndef on_send_error(excp):\n    log.error('I am an errback', exc_info=excp)\n    # handle exception\n\nif __name__ == \"__main__\":\n    run()\n</code></pre> <p>The primary and most significant modification to the current EMQX subscriber logic is that, in addition to printing the message in the subscribe function, we are now sending it to the Kafka producer as seen below.</p> <pre><code>def subscribe(client: mqtt_client):\n    def on_message(client, userdata, msg):\n        print(f\"Received `{msg.payload.decode()}` from `{msg.topic}` topic\")\n        producer.send('emqx-to-kafka', {\"data\": msg.payload.decode()}).add_callback(on_send_success).add_errback(on_send_error)\n        # block until all async messages are sent\n        producer.flush()\n</code></pre> <p>In the end, kafka consumer will receive a message sent by emqx publisher.</p> <p></p> <p>Therefore, in a real-time situation, the emqx publisher logic will be implemented in IOT devices, which will regularly transmit messages and record real-time data. The data will then be processed by kafka consumers, and we will be able to store and process the data in a backend database.</p> <p>We will look at storing the messages from the Kafka consumer into the Mango database in the following section.</p>"},{"location":"blog/2023/11/13/emqx-and-kafka-app/#processing-and-storing-real-time-data-into-mongodb","title":"Processing and storing real time data into mongodb","text":"<p>In this section, we are going to create a python function using pymongo, a python library to interact with mongo db. we can create a free account @ https://www.mongodb.com/. By default, we can create a free standard project in mongodb cloud console without paying anything. though there are some restrictions but for a beginer its obsolutely fine to get start.</p> <p>I am not going to explain how to create cluster/project/database/collection in mongodb in this turotial but i will create a separate document for it in future.</p> <p>1) Below is my mongo handler which will connect to database and store the messages into \"incoming\" collection.</p> <pre><code>from pymongo import MongoClient\n\ndef MongoInsertHandler(doc):\n    uri             = \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;mongo-endpoint&gt;/?retryWrites=true&amp;w=majority\"\n    collection_name = \"incoming\"\n\n    try:\n        client = MongoClient(uri)\n    # return a friendly error if a URI error is thrown \n    except Exception as e:\n        print(e)\n\n    # use a database\n    db = client[\"emqx-kafka-mongo-app\"]\n\n    # use a collection\n    my_collection = db[collection_name]\n\n    # INSERT DOCUMENTS\n    #\n    # In this example, You can insert individual documents using collection.insert_one().\n    # If you are creating multiple documents then we can insert them all with insert_many().\n    try:\n        print(\"Inserting document: {}\".format(doc))\n        result = my_collection.insert_one(doc)\n        print(\"Document inserted!\")\n    # return a friendly error if the operation fails\n    except Exception as e:\n        print(e)\n\n# if __name__ == \"__main__\":\n#     document = {\"hello\": \"HI\"}\n#     MongoInsertHandler(document)\n</code></pre> <p>2) As a next step, I will integrate this mongo handler into kafka consumer. When kafka consumer receives message from kafka producer, it will insert that message into mongodb. before calling the mongo handler, I am formatting the data into a json format.</p> <pre><code>import json\nfrom kafka import KafkaConsumer\nfrom mongo_handler import MongoInsertHandler\n\n# To consume latest messages and auto-commit offsets\n# consume json messages\n# StopIteration if no message after 1sec\n# auto_offset_reset='earliest', enable_auto_commit=False\nconsumer = KafkaConsumer(bootstrap_servers='localhost:9092',\n                         auto_offset_reset='latest', \n                         enable_auto_commit=True,\n                         consumer_timeout_ms=1000,\n                         value_deserializer=lambda m: json.loads(m.decode('ascii')))\n\n# Subscribe to a regex topic pattern\nconsumer.subscribe(pattern='^emqx.*')\n\nwhile True:\n    for message in consumer:\n        # message value and key are raw bytes -- decode if necessary!\n        # e.g., for unicode: `message.value.decode('utf-8')`\n        print (\"%s:%d:%d: data=%s\" % (message.topic, message.partition,\n                                            message.offset, message.value))\n        document = {\"topic\": message.topic, \"partition\": message.partition, \"offset\": message.offset, \"value\": message.value}\n        MongoInsertHandler(document)\n</code></pre> <p>In the existing kafka consumer logic, I have appended the below lines in a while loop.</p> <pre><code>document = {\"topic\": message.topic, \"partition\": message.partition, \"offset\": message.offset, \"value\": message.value}\nMongoInsertHandler(document)\n</code></pre> <p> As you can see on screen 3, document is getting inserted up on receiving in kafka consumer. In the next image, you can also find the records inserted in the collection in the mongodb cloud console. </p>  *** End of Tutorial ***"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""}]}